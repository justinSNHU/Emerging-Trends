{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import cifar10\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.optimizers import SGD, Adam, RMSprop\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ssl\n",
    "ssl._create_default_https_context=ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CIFAR_10 is a set of 60K images 32x32 pixels on 3 channels\n",
    "IMG_CHANNELS = 3\n",
    "IMG_ROWS = 32\n",
    "IMG_COLS = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#constant\n",
    "BATCH_SIZE = 128\n",
    "NB_EPOCH = 40\n",
    "NB_CLASSES = 10\n",
    "VERBOSE = 1\n",
    "VALIDATION_SPLIT = 0.2\n",
    "OPTIM = RMSprop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "170500096/170498071 [==============================] - 3s 0us/step\n",
      "X_train shape: (50000, 32, 32, 3)\n",
      "50000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "#load dataset\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "print('X_train shape:', X_train.shape)\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to categorical\n",
    "Y_train = np_utils.to_categorical(y_train, NB_CLASSES)\n",
    "Y_test = np_utils.to_categorical(y_test, NB_CLASSES)\n",
    "# float and normalization\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 32, 32, 32)        896       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 32, 32, 32)        9248      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 16, 16, 64)        18496     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 14, 14, 64)        36928     \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 3136)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               1606144   \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                5130      \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 1,676,842\n",
      "Trainable params: 1,676,842\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:12: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3))`\n",
      "  if sys.path[0] == '':\n"
     ]
    }
   ],
   "source": [
    "# network\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), padding='same',\n",
    "input_shape=(IMG_ROWS, IMG_COLS, IMG_CHANNELS)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(32, (3, 3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(64, 3, 3))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(NB_CLASSES))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/40\n",
      "40000/40000 [==============================] - 108s 3ms/step - loss: 1.7796 - accuracy: 0.3542 - val_loss: 1.4937 - val_accuracy: 0.4477\n",
      "Epoch 2/40\n",
      "40000/40000 [==============================] - 111s 3ms/step - loss: 1.3467 - accuracy: 0.5186 - val_loss: 1.1867 - val_accuracy: 0.5816\n",
      "Epoch 3/40\n",
      "40000/40000 [==============================] - 116s 3ms/step - loss: 1.1336 - accuracy: 0.5992 - val_loss: 0.9841 - val_accuracy: 0.6553\n",
      "Epoch 4/40\n",
      "40000/40000 [==============================] - 110s 3ms/step - loss: 0.9974 - accuracy: 0.6497 - val_loss: 0.8782 - val_accuracy: 0.6918\n",
      "Epoch 5/40\n",
      "40000/40000 [==============================] - 111s 3ms/step - loss: 0.9018 - accuracy: 0.6846 - val_loss: 1.0078 - val_accuracy: 0.6476\n",
      "Epoch 6/40\n",
      "40000/40000 [==============================] - 111s 3ms/step - loss: 0.8276 - accuracy: 0.7091 - val_loss: 0.8270 - val_accuracy: 0.7104\n",
      "Epoch 7/40\n",
      "40000/40000 [==============================] - 107s 3ms/step - loss: 0.7706 - accuracy: 0.7316 - val_loss: 0.8158 - val_accuracy: 0.7130\n",
      "Epoch 8/40\n",
      "40000/40000 [==============================] - 115s 3ms/step - loss: 0.7238 - accuracy: 0.7487 - val_loss: 0.7614 - val_accuracy: 0.7352\n",
      "Epoch 9/40\n",
      "40000/40000 [==============================] - 111s 3ms/step - loss: 0.6787 - accuracy: 0.7650 - val_loss: 0.7087 - val_accuracy: 0.7607\n",
      "Epoch 10/40\n",
      "40000/40000 [==============================] - 110s 3ms/step - loss: 0.6485 - accuracy: 0.7750 - val_loss: 0.7016 - val_accuracy: 0.7584\n",
      "Epoch 11/40\n",
      "40000/40000 [==============================] - 123s 3ms/step - loss: 0.6212 - accuracy: 0.7851 - val_loss: 0.8073 - val_accuracy: 0.7338\n",
      "Epoch 12/40\n",
      "40000/40000 [==============================] - 111s 3ms/step - loss: 0.6036 - accuracy: 0.7934 - val_loss: 0.7945 - val_accuracy: 0.7411\n",
      "Epoch 13/40\n",
      "40000/40000 [==============================] - 110s 3ms/step - loss: 0.5808 - accuracy: 0.8021 - val_loss: 0.7912 - val_accuracy: 0.7338\n",
      "Epoch 14/40\n",
      "40000/40000 [==============================] - 110s 3ms/step - loss: 0.5675 - accuracy: 0.8073 - val_loss: 0.7030 - val_accuracy: 0.7682\n",
      "Epoch 15/40\n",
      "40000/40000 [==============================] - 110s 3ms/step - loss: 0.5622 - accuracy: 0.8101 - val_loss: 0.7256 - val_accuracy: 0.7718\n",
      "Epoch 16/40\n",
      "40000/40000 [==============================] - 115s 3ms/step - loss: 0.5573 - accuracy: 0.8125 - val_loss: 0.7470 - val_accuracy: 0.7749\n",
      "Epoch 17/40\n",
      "40000/40000 [==============================] - 108s 3ms/step - loss: 0.5467 - accuracy: 0.8166 - val_loss: 0.8655 - val_accuracy: 0.7690\n",
      "Epoch 18/40\n",
      "40000/40000 [==============================] - 108s 3ms/step - loss: 0.5500 - accuracy: 0.8169 - val_loss: 0.8466 - val_accuracy: 0.7629\n",
      "Epoch 19/40\n",
      "40000/40000 [==============================] - 116s 3ms/step - loss: 0.5422 - accuracy: 0.8209 - val_loss: 0.7413 - val_accuracy: 0.7891\n",
      "Epoch 20/40\n",
      "40000/40000 [==============================] - 113s 3ms/step - loss: 0.5423 - accuracy: 0.8224 - val_loss: 0.8021 - val_accuracy: 0.7762\n",
      "Epoch 21/40\n",
      "40000/40000 [==============================] - 112s 3ms/step - loss: 0.5326 - accuracy: 0.8238 - val_loss: 0.8748 - val_accuracy: 0.7512\n",
      "Epoch 22/40\n",
      "40000/40000 [==============================] - 115s 3ms/step - loss: 0.5377 - accuracy: 0.8225 - val_loss: 0.6632 - val_accuracy: 0.7882\n",
      "Epoch 23/40\n",
      "40000/40000 [==============================] - 109s 3ms/step - loss: 0.5339 - accuracy: 0.8241 - val_loss: 0.7799 - val_accuracy: 0.7775\n",
      "Epoch 24/40\n",
      "40000/40000 [==============================] - 114s 3ms/step - loss: 0.5335 - accuracy: 0.8255 - val_loss: 0.7238 - val_accuracy: 0.7661\n",
      "Epoch 25/40\n",
      "40000/40000 [==============================] - 107s 3ms/step - loss: 0.5338 - accuracy: 0.8272 - val_loss: 0.7700 - val_accuracy: 0.7623\n",
      "Epoch 26/40\n",
      "40000/40000 [==============================] - 107s 3ms/step - loss: 0.5359 - accuracy: 0.8260 - val_loss: 0.8467 - val_accuracy: 0.7800\n",
      "Epoch 27/40\n",
      "40000/40000 [==============================] - 114s 3ms/step - loss: 0.5335 - accuracy: 0.8267 - val_loss: 0.7922 - val_accuracy: 0.7841\n",
      "Epoch 28/40\n",
      "40000/40000 [==============================] - 108s 3ms/step - loss: 0.5356 - accuracy: 0.8278 - val_loss: 0.9976 - val_accuracy: 0.7659\n",
      "Epoch 29/40\n",
      "40000/40000 [==============================] - 109s 3ms/step - loss: 0.5289 - accuracy: 0.8309 - val_loss: 0.8126 - val_accuracy: 0.7829\n",
      "Epoch 30/40\n",
      "40000/40000 [==============================] - 112s 3ms/step - loss: 0.5367 - accuracy: 0.8260 - val_loss: 0.7410 - val_accuracy: 0.7796\n",
      "Epoch 31/40\n",
      "40000/40000 [==============================] - 106s 3ms/step - loss: 0.5498 - accuracy: 0.8251 - val_loss: 0.8040 - val_accuracy: 0.7682\n",
      "Epoch 32/40\n",
      "40000/40000 [==============================] - 110s 3ms/step - loss: 0.5418 - accuracy: 0.8268 - val_loss: 0.9135 - val_accuracy: 0.7575\n",
      "Epoch 33/40\n",
      "40000/40000 [==============================] - 108s 3ms/step - loss: 0.5426 - accuracy: 0.8256 - val_loss: 0.8261 - val_accuracy: 0.7516\n",
      "Epoch 34/40\n",
      "40000/40000 [==============================] - 107s 3ms/step - loss: 0.5407 - accuracy: 0.8275 - val_loss: 0.8122 - val_accuracy: 0.7723\n",
      "Epoch 35/40\n",
      "40000/40000 [==============================] - 121s 3ms/step - loss: 0.5464 - accuracy: 0.8255 - val_loss: 0.9657 - val_accuracy: 0.7420\n",
      "Epoch 36/40\n",
      "40000/40000 [==============================] - 107s 3ms/step - loss: 0.5446 - accuracy: 0.8250 - val_loss: 0.7090 - val_accuracy: 0.7838\n",
      "Epoch 37/40\n",
      "40000/40000 [==============================] - 119s 3ms/step - loss: 0.5431 - accuracy: 0.8248 - val_loss: 0.8519 - val_accuracy: 0.7740\n",
      "Epoch 38/40\n",
      "40000/40000 [==============================] - 116s 3ms/step - loss: 0.5451 - accuracy: 0.8260 - val_loss: 0.8436 - val_accuracy: 0.7801\n",
      "Epoch 39/40\n",
      "40000/40000 [==============================] - 108s 3ms/step - loss: 0.5526 - accuracy: 0.8248 - val_loss: 0.7657 - val_accuracy: 0.7905\n",
      "Epoch 40/40\n",
      "40000/40000 [==============================] - 113s 3ms/step - loss: 0.5515 - accuracy: 0.8232 - val_loss: 0.8444 - val_accuracy: 0.7913\n",
      "10000/10000 [==============================] - 12s 1ms/step\n",
      "Test score: 0.8695285415649414\n",
      "Test accuracy: 0.7821000218391418\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "model.compile(loss='categorical_crossentropy', optimizer=OPTIM,\n",
    "metrics=['accuracy'])\n",
    "model.fit(X_train, Y_train, batch_size=BATCH_SIZE,\n",
    "epochs=NB_EPOCH, validation_split=VALIDATION_SPLIT,\n",
    "verbose=VERBOSE)\n",
    "score = model.evaluate(X_test, Y_test,\n",
    "batch_size=BATCH_SIZE, verbose=VERBOSE)\n",
    "print(\"Test score:\", score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save model\n",
    "model_json = model.to_json()\n",
    "open('cifar10_architecture.json', 'w').write(model_json)\n",
    "# And the weights learned by our deep network on the training set\n",
    "model.save_weights('cifar10_weights.h5', overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (50000, 32, 32, 3)\n",
      "50000 train samples\n",
      "10000 test samples\n",
      "Augmenting training set images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:44: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<keras.pre..., epochs=50, verbose=1, steps_per_epoch=390)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "390/390 [==============================] - 163s 417ms/step - loss: 2.3027 - accuracy: 0.0990\n",
      "Epoch 2/50\n",
      "390/390 [==============================] - 129s 330ms/step - loss: 2.3027 - accuracy: 0.0989\n",
      "Epoch 3/50\n",
      "390/390 [==============================] - 129s 331ms/step - loss: 2.3027 - accuracy: 0.1001\n",
      "Epoch 4/50\n",
      "390/390 [==============================] - 115s 294ms/step - loss: 2.3027 - accuracy: 0.0971\n",
      "Epoch 5/50\n",
      "390/390 [==============================] - 119s 304ms/step - loss: 2.3027 - accuracy: 0.0966\n",
      "Epoch 6/50\n",
      "390/390 [==============================] - 164s 420ms/step - loss: 2.3027 - accuracy: 0.0990\n",
      "Epoch 7/50\n",
      "390/390 [==============================] - 141s 362ms/step - loss: 2.3027 - accuracy: 0.0989\n",
      "Epoch 8/50\n",
      "390/390 [==============================] - 181s 464ms/step - loss: 2.3027 - accuracy: 0.0974\n",
      "Epoch 9/50\n",
      "390/390 [==============================] - 175s 449ms/step - loss: 2.3027 - accuracy: 0.0987\n",
      "Epoch 10/50\n",
      "390/390 [==============================] - 175s 448ms/step - loss: 2.3027 - accuracy: 0.0983\n",
      "Epoch 11/50\n",
      "390/390 [==============================] - 191s 490ms/step - loss: 2.3027 - accuracy: 0.0987\n",
      "Epoch 12/50\n",
      "390/390 [==============================] - 179s 458ms/step - loss: 2.3027 - accuracy: 0.0958\n",
      "Epoch 13/50\n",
      "390/390 [==============================] - 178s 455ms/step - loss: 2.3027 - accuracy: 0.0976\n",
      "Epoch 14/50\n",
      "390/390 [==============================] - 183s 469ms/step - loss: 2.3027 - accuracy: 0.0995\n",
      "Epoch 15/50\n",
      "390/390 [==============================] - 177s 453ms/step - loss: 2.3027 - accuracy: 0.1003\n",
      "Epoch 16/50\n",
      "390/390 [==============================] - 179s 458ms/step - loss: 2.3027 - accuracy: 0.0985\n",
      "Epoch 17/50\n",
      "390/390 [==============================] - 175s 450ms/step - loss: 2.3027 - accuracy: 0.0982\n",
      "Epoch 18/50\n",
      "390/390 [==============================] - 180s 462ms/step - loss: 2.3027 - accuracy: 0.0977\n",
      "Epoch 19/50\n",
      "390/390 [==============================] - 178s 457ms/step - loss: 2.3027 - accuracy: 0.0971\n",
      "Epoch 20/50\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 2.3027 - accuracy: 0.0984\n",
      "Epoch 21/50\n",
      "390/390 [==============================] - 178s 456ms/step - loss: 2.3027 - accuracy: 0.0959\n",
      "Epoch 22/50\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 2.3027 - accuracy: 0.0983\n",
      "Epoch 23/50\n",
      "390/390 [==============================] - 177s 455ms/step - loss: 2.3028 - accuracy: 0.0970\n",
      "Epoch 24/50\n",
      "390/390 [==============================] - 178s 455ms/step - loss: 2.2773 - accuracy: 0.1217\n",
      "Epoch 25/50\n",
      "390/390 [==============================] - 175s 448ms/step - loss: 1.9602 - accuracy: 0.2735\n",
      "Epoch 26/50\n",
      "390/390 [==============================] - 179s 459ms/step - loss: 1.6764 - accuracy: 0.4046\n",
      "Epoch 27/50\n",
      "390/390 [==============================] - 148s 380ms/step - loss: 1.5458 - accuracy: 0.4611\n",
      "Epoch 28/50\n",
      "390/390 [==============================] - 123s 316ms/step - loss: 1.4855 - accuracy: 0.4838\n",
      "Epoch 29/50\n",
      "390/390 [==============================] - 114s 292ms/step - loss: 1.4546 - accuracy: 0.4958\n",
      "Epoch 30/50\n",
      "390/390 [==============================] - 111s 284ms/step - loss: 1.4354 - accuracy: 0.5050\n",
      "Epoch 31/50\n",
      "390/390 [==============================] - 119s 306ms/step - loss: 1.4078 - accuracy: 0.5144\n",
      "Epoch 32/50\n",
      "390/390 [==============================] - 120s 308ms/step - loss: 1.3873 - accuracy: 0.5229\n",
      "Epoch 33/50\n",
      "390/390 [==============================] - 119s 305ms/step - loss: 1.3810 - accuracy: 0.5222\n",
      "Epoch 34/50\n",
      "390/390 [==============================] - 163s 419ms/step - loss: 1.3719 - accuracy: 0.5254\n",
      "Epoch 35/50\n",
      "390/390 [==============================] - 183s 470ms/step - loss: 1.3707 - accuracy: 0.5302\n",
      "Epoch 36/50\n",
      "390/390 [==============================] - 176s 451ms/step - loss: 1.3570 - accuracy: 0.5321\n",
      "Epoch 37/50\n",
      "390/390 [==============================] - 180s 462ms/step - loss: 1.3610 - accuracy: 0.5333\n",
      "Epoch 38/50\n",
      "390/390 [==============================] - 179s 460ms/step - loss: 1.3486 - accuracy: 0.5355\n",
      "Epoch 39/50\n",
      "390/390 [==============================] - 188s 483ms/step - loss: 1.3463 - accuracy: 0.5379\n",
      "Epoch 40/50\n",
      "390/390 [==============================] - 185s 473ms/step - loss: 1.3406 - accuracy: 0.5392\n",
      "Epoch 41/50\n",
      "390/390 [==============================] - 180s 461ms/step - loss: 1.3443 - accuracy: 0.5387\n",
      "Epoch 42/50\n",
      "390/390 [==============================] - 182s 468ms/step - loss: 1.3321 - accuracy: 0.5427\n",
      "Epoch 43/50\n",
      "390/390 [==============================] - 184s 471ms/step - loss: 1.3341 - accuracy: 0.5455\n",
      "Epoch 44/50\n",
      "390/390 [==============================] - 180s 463ms/step - loss: 1.3346 - accuracy: 0.5428\n",
      "Epoch 45/50\n",
      "390/390 [==============================] - 183s 468ms/step - loss: 1.3397 - accuracy: 0.5418\n",
      "Epoch 46/50\n",
      "390/390 [==============================] - 178s 457ms/step - loss: 1.3372 - accuracy: 0.5447\n",
      "Epoch 47/50\n",
      "390/390 [==============================] - 183s 469ms/step - loss: 1.3350 - accuracy: 0.5435\n",
      "Epoch 48/50\n",
      "390/390 [==============================] - 184s 472ms/step - loss: 1.3347 - accuracy: 0.5438\n",
      "Epoch 49/50\n",
      "390/390 [==============================] - 178s 455ms/step - loss: 1.3339 - accuracy: 0.5425\n",
      "Epoch 50/50\n",
      "390/390 [==============================] - 192s 492ms/step - loss: 1.3346 - accuracy: 0.5465\n",
      "10000/10000 [==============================] - 14s 1ms/step\n",
      "Test score: 1.2220415321350098\n",
      "Test accuracy: 0.5954999923706055\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.datasets import cifar10\n",
    "import numpy as np\n",
    "NUM_TO_AUGMENT=5\n",
    "NB_EPOCH=50\n",
    "#load dataset\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "print('X_train shape:', X_train.shape)\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')\n",
    "# convert to categorical\n",
    "Y_train = np_utils.to_categorical(y_train, NB_CLASSES)\n",
    "Y_test = np_utils.to_categorical(y_test, NB_CLASSES)\n",
    "# float and normalization\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "# augumenting\n",
    "print(\"Augmenting training set images...\")\n",
    "datagen = ImageDataGenerator(\n",
    "rotation_range=40,\n",
    "width_shift_range=0.2,\n",
    "height_shift_range=0.2,\n",
    "zoom_range=0.2,\n",
    "horizontal_flip=True,\n",
    "fill_mode='nearest')\n",
    "xtas, ytas = [], []\n",
    "for i in range(X_train.shape[0]):\n",
    "    num_aug = 0\n",
    "    x = X_train[i] # (3, 32, 32)\n",
    "    x = x.reshape((1,) + x.shape) # (1, 3, 32, 32)   \n",
    "for x_aug in datagen.flow(x, batch_size=1,\n",
    "save_to_dir='preview', save_prefix='cifar', save_format='jpeg'):\n",
    "    if num_aug >= NUM_TO_AUGMENT:\n",
    "        break\n",
    "    xtas.append(x_aug[0])\n",
    "    num_aug += 1\n",
    "#fit the dataget\n",
    "datagen.fit(X_train)\n",
    "# train\n",
    "history = model.fit_generator(datagen.flow(X_train, Y_train,\n",
    "batch_size=BATCH_SIZE), samples_per_epoch=X_train.shape[0],\n",
    "epochs=NB_EPOCH, verbose=VERBOSE)\n",
    "score = model.evaluate(X_test, Y_test,\n",
    "batch_size=BATCH_SIZE, verbose=VERBOSE)\n",
    "print(\"Test score:\", score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today, the use of Convolutional Neural Networks (CNNs) is becoming more prevalent, and these deep-learning algorithms are<br> extraordinary at being able to identify complex patterns within images and videos, making them essential in today’s image<br> recognition tasks (Visionplatform.ai, 2024).<br><br>  \n",
    "Since the way algorithms are trained is based on the data set used, these types of algorithms could certainly be used to<br> distinguish people’s faces (Visionplatform.ai, 2024). Once an algorithm is trained, and equipped with what it has learned from <br>the data set, it can analyze new images by breaking down each image into component elements, which are often pixels, where <br>the algorithm can then look for patterns and features it has been previously trained to recognize (Visionplatform.ai, 2024).<br><br>\n",
    "Therefore, we can begin to better understand how we can feed any dataset to the algorithm, which can then be used to train it,<br> and then decide what we want to do with that trained algorithm, which includes facial recognition among a wide variety of <br>other uses of image recognition.<br><br>\n",
    "Because it is possible to train algorithms for image recognition, such as facial recognition, it is significant to have an <br>understanding of its ethical and privacy implications.<br><br>\n",
    "Ethical and privacy concerns that arise from facial recognition include, Racial bias due to testing inaccuracies, <br>discrimination in law enforcement, data privacy, lack of informed consent and transparency, mass surveillance, data <br>breaches, and lack of legal support (Gangarapu, 2022).<br><br>\n",
    "As it concerns racial bias due to testing inaccuracies, it has been common that of the errors detected in facial recognition<br> systems used for law enforcement, errors were more common on dark-skinned faces, and it further showed fewer<br> errors when matching light-skinned faces (Gangarapu, 2022). Also concerning issues in law enforcement, it has been<br> found that facial recognition algorithms worked better for faces of middle-aged white males, but underperformed for <br>people of color (specifically 35% of errors occurred with women of color compared to 1% for white males), elderly people,<br> and women and children (Gangarapu, 2022). Overall, facial recognition could potentially result in penalizing individuals<br> for crimes they did not actually commit (Gangarapu, 2022).<br><br>\n",
    "Privacy is a significant public concern regarding facial recognition because it “infringes on citizens’ inherent right to be<br> under constant government surveillance and keep their images without consent” (Gangarapu, 2022). This also means that<br> facial recognition can lead to mass surveillance, which compromises the liberty and privacy rights of citizens when<br> used alongside cameras that are everywhere and in conjunction with data analytics (Gangarapu, 2022).<br><br>\n",
    "Also, when considering privacy, it is important to realize that many issues arise from unsecured data storage methods, which <br>expose this type of data and other security-related threats. Data breaches are of significant concern because they can lead<br> to facial identity in serious crimes; quite often leaving victims with very few legal remedies (Gangarapu, 2022). <br><br>\n",
    "Lastly, the methods by which facial images are often collected are from publicly hosted sites, such as Facebook, where people<br> have openly shared their pictures, however, these images are found to be misused to help assess and advance <br>commercial surveillance products (Gangarapu, 2022). <br><br> \n",
    "References:<br><br>\n",
    "Gangarapu, K. R. (2022, January 25). Ethics of Facial Recognition: Key Issues and Solutions. G2.com, Inc..<br> https://learn.g2.com/ethics-of-facial-recognition<br><br>\n",
    "Visionplatform.ai. (2024, January 27). Understanding Image Recognition: Algorithms, Machine Learning, and Uses.<br> https://visionplatform.ai/image-recognition/\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
